from pyneuphonic import Neuphonic, WebsocketEvents
from pyneuphonic.player import AsyncAudioPlayer, AsyncAudioRecorder
from pyneuphonic.models import APIResponse, AgentResponse, AgentConfig
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM


import os
import asyncio


async def main():
    # Initialize variables
    transcript = ""
    client = Neuphonic(api_key="172c4d78863aec707d4496f0d5258bff3389e92b2614c8175e7f18575eebc4a5.f13b6de5-100a-4252-b2fd-7a4aa39c852f")

    ws = client.agents.AsyncWebsocketClient()
    player = AsyncAudioPlayer()
    recorder = AsyncAudioRecorder(websocket=ws)  # Auto-forward audio to server

    async def on_message(message: APIResponse[AgentResponse]):
        nonlocal transcript  # Allow modification of the outer transcript variable
        if message.data.type == 'audio_response':
            await player.play(message.data.audio)
        elif message.data.type == 'user_transcript':
            # Print the user's transcription
            print(f"User: {message.data.text}")

            # Accumulate the transcription
            transcript += message.data.text + " "  # Add space for better separation of sentences
            
            # Process the transcript with LLM only after it is updated
            print("\nProcessing transcript with LLM...")
            
            # Ensure the tokenizer and model are correctly loaded
            try:
                tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B",use_auth_token=True)
                model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B",use_auth_token=True)

            except Exception as e:
                print(f"Error loading tokenizer/model: {e}")
                return

            # Prepare the input for LLM
            input_text = f"Find the food item in this respective phrase and print it: {transcript.strip()}"
            print(f"Input to LLM: {input_text}")  # Print the input to LLM to debug

            inputs = tokenizer(input_text, return_tensors="pt")
            
            try:
                # Generate the response from the LLM
                outputs = model.generate(inputs["input_ids"], max_length=50)
            except Exception as e:
                print(f"Error generating output: {e}")
                return

            # Check if the output is empty or not
            if outputs is not None and len(outputs) > 0:
                # Decode the generated output and print it
                decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print("LLM Output:")
                print(decoded_output)
            else:
                print("No output generated by the LLM.")
        elif message.data.type == 'llm_response':
                    print(f"Agent: {message.data.text}")

    async def on_close():
        await player.close()
        await recorder.close()

    ws.on(WebsocketEvents.MESSAGE, on_message)
    ws.on(WebsocketEvents.CLOSE, on_close)

    await player.open()
    await ws.open(AgentConfig(endpointing=500))
    await recorder.record()

    try:
        print("Recording... Press Ctrl+C to stop.")
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("\nStopping recording...")
        await ws.close()

   


# Run the main coroutine
asyncio.run(main())
